{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5116a63e",
   "metadata": {},
   "source": [
    "Manan Pujara\n",
    "\n",
    "Data source : https://www.kaggle.com/datasets/gauravpathak1789/yellow-tripdata-2020-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/13 21:19:11 WARN Utils: Your hostname, Dexter, resolves to a loopback address: 127.0.1.1; using 10.20.67.112 instead (on interface wlo1)\n",
      "25/07/13 21:19:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/13 21:19:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"week8app\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f5a8fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.20.67.112:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>week8app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa16ef8d6a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1226f21",
   "metadata": {},
   "source": [
    "## Load CSV into PySpark DataFrame\n",
    "\n",
    "**I'm using local notebook + pyspark library having issue with databricks account (and also i m out of free credits and subscription for azure databricks)**\n",
    "\n",
    "- so for databricks goto catalog and there you can choose workspace and add schema or in existing schema you can create volume then upload csv then copy path and mount it \n",
    "image for better understanding \n",
    "\n",
    "<center><img src=\"./image.png\" height=\"500\" width=\"1000\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5cd8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b640b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2020-01-01 00:28:15|  2020-01-01 00:33:03|              1|          1.2|         1|                 N|         238|         239|           1|        6.0|  3.0|    0.5|      1.47|         0.0|                  0.3|       11.27|                 2.5|\n",
      "|       1| 2020-01-01 00:35:39|  2020-01-01 00:43:04|              1|          1.2|         1|                 N|         239|         238|           1|        7.0|  3.0|    0.5|       1.5|         0.0|                  0.3|        12.3|                 2.5|\n",
      "|       1| 2020-01-01 00:47:41|  2020-01-01 00:53:52|              1|          0.6|         1|                 N|         238|         238|           1|        6.0|  3.0|    0.5|       1.0|         0.0|                  0.3|        10.8|                 2.5|\n",
      "|       1| 2020-01-01 00:55:23|  2020-01-01 01:00:14|              1|          0.8|         1|                 N|         238|         151|           1|        5.5|  0.5|    0.5|      1.36|         0.0|                  0.3|        8.16|                 0.0|\n",
      "|       2| 2020-01-01 00:01:58|  2020-01-01 00:04:16|              1|          0.0|         1|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         4.8|                 0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True)\\\n",
    "    .csv(\"./yellow_tripdata_2020-01 (1).csv\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb2d5dc",
   "metadata": {},
   "source": [
    "## Clean/Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6fcb191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_cast = ['fare_amount', 'extra', 'mta_tax', 'tip_amount',\n",
    "                'tolls_amount', 'improvement_surcharge', 'total_amount']\n",
    "\n",
    "for col_name in columns_to_cast:\n",
    "    df = df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "\n",
    "df = df.withColumn(\"tpep_pickup_datetime\", to_timestamp(\"tpep_pickup_datetime\")) \\\n",
    "       .withColumn(\"tpep_dropoff_datetime\", to_timestamp(\"tpep_dropoff_datetime\"))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e8f4cf",
   "metadata": {},
   "source": [
    "## Flatten JSON fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "# Defining Json Field for example \n",
    "schema = StructType([\n",
    "    StructField(\"trip_id\", IntegerType()),\n",
    "    StructField(\"driver\", StructType([\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"license\", StringType())\n",
    "    ])),\n",
    "    StructField(\"fare\", StructType([\n",
    "        StructField(\"amount\", DoubleType()),\n",
    "        StructField(\"tip\", DoubleType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# simulating json field for example here \n",
    "json_data = [\n",
    "    ('{\"trip_id\":101,\"driver\":{\"name\":\"Manan Pujara\",\"license\":\"XYZ1234\"},\"fare\":{\"amount\":25.0,\"tip\":3.0}}',)\n",
    "]\n",
    "\n",
    "df_raw = spark.createDataFrame(json_data, [\"json_col\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----------+--------+\n",
      "|trip_id|driver_name |driver_license|fare_amount|fare_tip|\n",
      "+-------+------------+--------------+-----------+--------+\n",
      "|101    |Manan Pujara|XYZ1234       |25.0       |3.0     |\n",
      "+-------+------------+--------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting nested fields \n",
    "df_flat = df_raw.withColumn(\"parsed\", from_json(col(\"json_col\"), schema))\\\n",
    "    .select(\n",
    "        col(\"parsed.trip_id\"),\n",
    "        col(\"parsed.driver.name\").alias(\"driver_name\"),\n",
    "        col(\"parsed.driver.license\").alias(\"driver_license\"),\n",
    "        col(\"parsed.fare.amount\").alias(\"fare_amount\"),\n",
    "        col(\"parsed.fare.tip\").alias(\"fare_tip\")\n",
    "    )\n",
    "\n",
    "df_flat.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0bac56",
   "metadata": {},
   "source": [
    "## Query 1 : Add Revenue Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0db72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+----------+------------+-------+\n",
      "|fare_amount|extra|tip_amount|total_amount|Revenue|\n",
      "+-----------+-----+----------+------------+-------+\n",
      "|        6.0|  3.0|      1.47|       11.27|  22.54|\n",
      "|        7.0|  3.0|       1.5|        12.3|   24.6|\n",
      "|        6.0|  3.0|       1.0|        10.8|   21.6|\n",
      "|        5.5|  0.5|      1.36|        8.16|  16.32|\n",
      "|        3.5|  0.5|       0.0|         4.8|    9.6|\n",
      "+-----------+-----+----------+------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# making revenue column with addition of columns : fare_amount, extra, mta_tax, improvement_surcharge, tip_amount, tolls_amount, total_amount\n",
    "df = df.withColumn(\"Revenue\", \n",
    "    col(\"fare_amount\") + col(\"extra\") + col(\"mta_tax\") + \n",
    "    col(\"improvement_surcharge\") + col(\"tip_amount\") + \n",
    "    col(\"tolls_amount\") + col(\"total_amount\")\n",
    ")\n",
    "\n",
    "df.select(\"fare_amount\", \"extra\", \"tip_amount\", \"total_amount\", \"Revenue\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b758ced",
   "metadata": {},
   "source": [
    "## Query 2 : Passenger Count by Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91860e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                            (6 + 6) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+\n",
      "|PULocationID|Total_Passengers|\n",
      "+------------+----------------+\n",
      "|         237|          433243|\n",
      "|         161|          425986|\n",
      "|         236|          403347|\n",
      "|         230|          360096|\n",
      "|         162|          351011|\n",
      "|         186|          338952|\n",
      "|         132|          326402|\n",
      "|          48|          297148|\n",
      "|         142|          294502|\n",
      "|         170|          289593|\n",
      "+------------+----------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"PULocationID\")\\\n",
    "  .agg(sum(\"passenger_count\").alias(\"Total_Passengers\"))\\\n",
    "  .orderBy(desc(\"Total_Passengers\")).show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff00d91",
   "metadata": {},
   "source": [
    "## Query 3 : Real-time Average Fare/Earning by Vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df6181ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+\n",
      "|VendorID|          Avg_Fare| Avg_Total_Earning|\n",
      "+--------+------------------+------------------+\n",
      "|    NULL|31.996404547606964|37.217091425863046|\n",
      "|       1|12.231264768274265|18.113429405184878|\n",
      "|       2| 12.62490775307597|18.648347164036302|\n",
      "+--------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# finding average fare and total_amount using avg() aggregate function and group by with vendor ID\n",
    "df.groupBy(\"VendorID\")\\\n",
    "  .agg(avg(\"fare_amount\").alias(\"Avg_Fare\"),\n",
    "       avg(\"total_amount\").alias(\"Avg_Total_Earning\"))\\\n",
    "  .orderBy(\"VendorID\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9701d0c5",
   "metadata": {},
   "source": [
    "## Query 4 : Moving Count of Payments by Payment Mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "342f78a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+------------+\n",
      "|payment_type|tpep_pickup_datetime|Moving_Count|\n",
      "+------------+--------------------+------------+\n",
      "|           2| 2003-01-01 00:07:17|           1|\n",
      "|           1| 2008-12-31 23:02:40|           1|\n",
      "|           1| 2008-12-31 23:02:50|           2|\n",
      "|           1| 2008-12-31 23:03:44|           3|\n",
      "|           1| 2008-12-31 23:03:48|           4|\n",
      "|           2| 2008-12-31 23:06:13|           2|\n",
      "|           2| 2008-12-31 23:17:15|           3|\n",
      "|           2| 2008-12-31 23:24:11|           4|\n",
      "|           1| 2008-12-31 23:34:13|           5|\n",
      "|           2| 2008-12-31 23:35:00|           5|\n",
      "+------------+--------------------+------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "payment_window = Window.partitionBy(\"payment_type\")\\\n",
    "                       .orderBy(\"tpep_pickup_datetime\")\\\n",
    "                       .rowsBetween(-10, 0)\n",
    "\n",
    "df.withColumn(\"Moving_Count\", count(\"*\").over(payment_window))\\\n",
    "  .select(\"payment_type\", \"tpep_pickup_datetime\", \"Moving_Count\")\\\n",
    "  .orderBy(\"tpep_pickup_datetime\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83738395",
   "metadata": {},
   "source": [
    "## Query 5 : Top 2 Gaining Vendors on a Specific Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7af7081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:==============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+----------------+------------------+\n",
      "|VendorID|     Total_Revenue|Total_Passengers|    Total_Distance|\n",
      "+--------+------------------+----------------+------------------+\n",
      "|       2| 2700441.549999132|          233339| 410271.7600000014|\n",
      "|       1|1319816.5300006857|           82508|190960.49999999945|\n",
      "+--------+------------------+----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "specific_date = \"2020-01-15\"\n",
    "\n",
    "df.filter(to_date(\"tpep_pickup_datetime\") == specific_date)\\\n",
    "  .groupBy(\"VendorID\")\\\n",
    "  .agg(\n",
    "      sum(\"total_amount\").alias(\"Total_Revenue\"),\n",
    "      sum(\"passenger_count\").alias(\"Total_Passengers\"),\n",
    "      sum(\"trip_distance\").alias(\"Total_Distance\")\n",
    "  )\\\n",
    "  .orderBy(desc(\"Total_Revenue\"))\\\n",
    "  .limit(2)\\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc886d6",
   "metadata": {},
   "source": [
    "## Query 6 : Route With Most Passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e35f13c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------------+\n",
      "|PULocationID|DOLocationID|Passenger_Count|\n",
      "+------------+------------+---------------+\n",
      "|         237|         236|          67885|\n",
      "+------------+------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"PULocationID\", \"DOLocationID\")\\\n",
    "  .agg(sum(\"passenger_count\").alias(\"Passenger_Count\"))\\\n",
    "  .orderBy(desc(\"Passenger_Count\"))\\\n",
    "  .limit(1)\\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1326a870",
   "metadata": {},
   "source": [
    "## Query 7 : Top Pickup Locations in Last 10 Seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a9dc909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|PULocationID|Recent_Passengers|\n",
      "+------------+-----------------+\n",
      "|          90|                1|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "latest_time = df.agg(max(\"tpep_pickup_datetime\")).first()[0]\n",
    "window_start = latest_time - expr(\"INTERVAL 10 seconds\")\n",
    "\n",
    "df.filter((col(\"tpep_pickup_datetime\") > window_start) & (col(\"tpep_pickup_datetime\") <= latest_time))\\\n",
    "  .groupBy(\"PULocationID\")\\\n",
    "  .agg(sum(\"passenger_count\").alias(\"Recent_Passengers\"))\\\n",
    "  .orderBy(desc(\"Recent_Passengers\"))\\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab17168",
   "metadata": {},
   "source": [
    "## Save as External Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ea3bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/13 21:36:23 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/07/13 21:36:23 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/07/13 21:36:23 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/07/13 21:36:23 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/07/13 21:36:23 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/07/13 21:36:31 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/07/13 21:36:31 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/07/13 21:36:31 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/07/13 21:36:31 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|           Revenue|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+------------------+\n",
      "|       1| 2020-01-01 00:28:15|  2020-01-01 00:33:03|              1|          1.2|         1|                 N|         238|         239|           1|        6.0|  3.0|    0.5|      1.47|         0.0|                  0.3|       11.27|                 2.5|             22.54|\n",
      "|       1| 2020-01-01 00:35:39|  2020-01-01 00:43:04|              1|          1.2|         1|                 N|         239|         238|           1|        7.0|  3.0|    0.5|       1.5|         0.0|                  0.3|        12.3|                 2.5|              24.6|\n",
      "|       1| 2020-01-01 00:47:41|  2020-01-01 00:53:52|              1|          0.6|         1|                 N|         238|         238|           1|        6.0|  3.0|    0.5|       1.0|         0.0|                  0.3|        10.8|                 2.5|              21.6|\n",
      "|       1| 2020-01-01 00:55:23|  2020-01-01 01:00:14|              1|          0.8|         1|                 N|         238|         151|           1|        5.5|  0.5|    0.5|      1.36|         0.0|                  0.3|        8.16|                 0.0|             16.32|\n",
      "|       2| 2020-01-01 00:01:58|  2020-01-01 00:04:16|              1|          0.0|         1|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         4.8|                 0.0|               9.6|\n",
      "|       2| 2020-01-01 00:09:44|  2020-01-01 00:10:37|              1|         0.03|         1|                 N|           7|         193|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|               7.6|\n",
      "|       2| 2020-01-01 00:39:25|  2020-01-01 00:39:29|              1|          0.0|         1|                 N|         193|         193|           1|        2.5|  0.5|    0.5|      0.01|         0.0|                  0.3|        3.81|                 0.0| 7.619999999999999|\n",
      "|       2| 2019-12-18 15:27:49|  2019-12-18 15:28:59|              1|          0.0|         5|                 N|         193|         193|           1|       0.01|  0.0|    0.0|       0.0|         0.0|                  0.3|        2.81|                 2.5|              3.12|\n",
      "|       2| 2019-12-18 15:30:35|  2019-12-18 15:31:35|              4|          0.0|         1|                 N|         193|         193|           1|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.3|                 2.5|              10.1|\n",
      "|       1| 2020-01-01 00:29:01|  2020-01-01 00:40:28|              2|          0.7|         1|                 N|         246|          48|           1|        8.0|  3.0|    0.5|      2.35|         0.0|                  0.3|       14.15|                 2.5|              28.3|\n",
      "|       1| 2020-01-01 00:55:11|  2020-01-01 01:12:03|              2|          2.4|         1|                 N|         246|          79|           1|       12.0|  3.0|    0.5|      1.75|         0.0|                  0.3|       17.55|                 2.5|              35.1|\n",
      "|       1| 2020-01-01 00:37:15|  2020-01-01 00:51:41|              1|          0.8|         1|                 N|         163|         161|           2|        9.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        13.3|                 2.5|              26.6|\n",
      "|       1| 2020-01-01 00:56:27|  2020-01-01 01:21:44|              1|          3.3|         1|                 N|         161|         144|           1|       17.0|  3.0|    0.5|      4.15|         0.0|                  0.3|       24.95|                 2.5|49.900000000000006|\n",
      "|       2| 2020-01-01 00:21:54|  2020-01-01 00:27:31|              1|         1.07|         1|                 N|          43|         239|           1|        6.0|  0.5|    0.5|      1.96|         0.0|                  0.3|       11.76|                 2.5|             21.02|\n",
      "|       2| 2020-01-01 00:38:01|  2020-01-01 01:15:21|              1|         7.76|         1|                 N|         143|          25|           1|       28.5|  0.5|    0.5|      4.84|         0.0|                  0.3|       37.14|                 2.5|             71.78|\n",
      "|       1| 2020-01-01 00:15:35|  2020-01-01 00:27:06|              3|          1.6|         1|                 N|         211|         234|           2|        9.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        12.8|                 2.5|              25.6|\n",
      "|       1| 2020-01-01 00:41:20|  2020-01-01 00:44:22|              1|          0.5|         1|                 Y|         234|          90|           1|        4.0|  3.0|    0.5|       1.0|         0.0|                  0.3|         8.8|                 2.5|              17.6|\n",
      "|       1| 2020-01-01 00:56:38|  2020-01-01 01:13:34|              1|          1.7|         1|                 N|         246|         142|           2|       11.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        15.3|                 2.5|              30.6|\n",
      "|       2| 2020-01-01 00:08:21|  2020-01-01 00:25:29|              1|         8.45|         1|                 N|         138|         216|           2|       24.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        25.8|                 0.0|              51.6|\n",
      "|       1| 2020-01-01 00:25:39|  2020-01-01 00:27:05|              1|          0.0|         1|                 N|         170|         162|           4|        3.0|  3.0|    0.5|       0.0|         0.0|                  0.3|         6.8|                 2.5|              13.6|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"yellow_taxi_2020_01_parquet/\")\n",
    "\n",
    "df_parquet = spark.read.parquet(\"yellow_taxi_2020_01_parquet/\")\n",
    "df_parquet.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6688dcbd",
   "metadata": {},
   "source": [
    "## Extra Query : What hour of the day has the most earnings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e8328de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:===================>                                     (4 + 8) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|hour|    Total_Earnings|\n",
      "+----+------------------+\n",
      "|  18| 8139717.280000629|\n",
      "|  17| 7664537.440000109|\n",
      "|  19| 7303387.380000077|\n",
      "|  16| 7169013.939999765|\n",
      "|  15| 7121991.509999728|\n",
      "|  14| 6932949.289999597|\n",
      "|  21| 6681671.489999872|\n",
      "|  20| 6662722.759999678|\n",
      "|  13| 6350768.439999287|\n",
      "|  12| 6051112.649999132|\n",
      "|  22| 6047689.889999723|\n",
      "|  11| 5476375.819998952|\n",
      "|   9| 5375311.279999266|\n",
      "|   8| 5322249.309999476|\n",
      "|  10|  5292959.88999899|\n",
      "|  23|4540154.6599994255|\n",
      "|   7| 4253480.069999366|\n",
      "|   0| 3326239.589999602|\n",
      "|   6|  2446269.25999986|\n",
      "|   1|2241891.5899997917|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"hour\", hour(\"tpep_pickup_datetime\"))\\\n",
    "  .groupBy(\"hour\")\\\n",
    "  .agg(sum(\"total_amount\").alias(\"Total_Earnings\"))\\\n",
    "  .orderBy(desc(\"Total_Earnings\"))\\\n",
    "  .show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
